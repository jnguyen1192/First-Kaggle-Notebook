{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTRE KERNEL :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Compréhension du problème lié au Business\n",
    "\n",
    "Toutes ces données : pourquoi faire ?\n",
    "\n",
    "Exemple :\n",
    "\n",
    "Une banque souhaite pouvoir automatiser la lecture des montants sur les chèques.\n",
    "Pour cela, nous sommes sollicités afin de fournir un programme capable de lire des chiffres écrits à la main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compréhension des données : Analyse des données d'entraînement\n",
    "\n",
    "Dans cette partie, on réalise les opérations à la main pour comprendre les données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import warnings\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Chargement des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Analyse des données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère dans un premier temps le pattern des données, présent dans le document d'entraînement \"train.csv\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande nous indique qu'il y a en tout 785 colonnes :\n",
    "\n",
    "- La 1ère colonne correspond à la colonne des étiquettes\n",
    "- Les autres colonnes correspondent à la suite des valeurs des pixels de chaque image\n",
    "\n",
    "Ainsi, chaque ligne du fichier, à l'exception de la 1ère qui correspond au pattern présenté ci-dessus, représente une image par la suite des valeurs de ses pixels. Les images font donc 784 pixels chacune.\n",
    "\n",
    "Le 1er élément de la ligne est l'étiquette, ici c'est un nombre qui correspond au digit (nombre) que représente l'image.\n",
    "Nous allons donc réaliser du Machine Learning supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation et description générale des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prévisualisation des données\n",
    "print(train_df.head())\n",
    "\n",
    "# Description des données\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que l'objet pandas qui a servi au chargement des données, retourne un objet de type Data Frame.\n",
    "Cette commande nous indique également qu'il y a 42000 lignes :\n",
    "\n",
    "- Comme dit précédemment, la 1ère est le pattern\n",
    "- Les 41999 autres lignes sont les images\n",
    "\n",
    "Les valeurs stockées sont toutes des entiers.\n",
    "\n",
    "Les 785 colonnes contiennent toutes des valeurs catégoriques (à l'exception de la première ligne qui contient les noms des colonnes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données présentent-elles des trous ou des valeurs non entières ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsOnlyIntegerValues(matrix):\n",
    "\n",
    "\tfor line in range(len(matrix)):\n",
    "\t\tfor col in range(len(matrix[0])):\n",
    "\t\t\ttry :\n",
    "\t\t\t\tint(matrix[line][col])\n",
    "\t\t\texcept :\n",
    "\t\t\t\treturn False\n",
    "\n",
    "\treturn True\n",
    "\n",
    "matrix = train_df.values.tolist()\n",
    "print(containsOnlyIntegerValues(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction nous apprend que nos données sont correctement formées et toutes la valeurs sont bien des entiers.\n",
    "Il n'y a pas de valeur manquante.\n",
    "\n",
    "Étudions maintenant la plage de données de chaque colonne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumnsRange(matrix):\n",
    "\n",
    "\toutput = []\n",
    "\n",
    "\tfor col in range(len(matrix[0])):\n",
    "\t\tcolRange = []\n",
    "\t\tfor line in range(len(matrix)):\n",
    "\t\t\tvalue = matrix[line][col]\n",
    "\t\t\tif not (value in colRange):\n",
    "\t\t\t\tcolRange.append(value)\n",
    "\t\t\tcolRange.sort()\n",
    "\n",
    "\t\toutput.append(colRange)\n",
    "\n",
    "\treturn output\n",
    "\n",
    "print(train_df.describe())\n",
    "print(getColumnsRange(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe ainsi que la 1ère colonne, la colonne des étiquette prend les valeurs suivantes : de 0 à 9\n",
    "\n",
    "Et les autres colonnes prennent les valeurs suivantes :  de 0 à 255\n",
    "Cela confirme donc que les données contiennent des images représentant les chiffres 0, 1, 2, ..., 9\n",
    "En d'autre terme une image représente bien un unique chiffre et ce chiffre peut être un 0, un 1, ... ou un 9.\n",
    "De plus, les images sont représentées en niveaux de gris et sont codées sur 8 bits.\n",
    "\n",
    "On peut donc visualiser la 2e image des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étudions maintenant la distribution des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDateDistribution(matrix):\n",
    "\t\n",
    "\tdict = {}\n",
    "\tnumberOfLines = len(matrix)\n",
    "\n",
    "\tfor line in range(numberOfLines):\n",
    "\t\tvalue = matrix[line][0]\n",
    "\n",
    "\t\tif value in dict:\n",
    "\t\t\toccurrences = dict[value]\n",
    "\t\t\toccurrences += 1\n",
    "\t\t\tdict[value] = occurrences\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tdict[value] = 1\n",
    "\n",
    "\tfor value in dict:\n",
    "\t\toccurrences = dict[value]\n",
    "\t\tpercentage = (occurrences * 100) / numberOfLines\n",
    "\t\tdict[value] = percentage\n",
    "\n",
    "\treturn dict\n",
    "\n",
    "dict = getDateDistribution(matrix)\n",
    "\n",
    "for i in dict:\n",
    "\tprint(\"Digit : \" + str(i) + \"\\t\" + str(dict[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digit 0 : 9.838095238095239%\n",
    "\n",
    "Digit 1 : 11.152380952380952%\n",
    "\n",
    "Digit 2 : 9.945238095238095%\n",
    "\n",
    "Digit 3 : 10.35952380952381%\n",
    "\n",
    "Digit 4 : 9.695238095238095%\n",
    "\n",
    "Digit 5 : 9.035714285714286%\n",
    "\n",
    "Digit 6 : 9.85%\n",
    "\n",
    "Digit 7 : 10.478571428571428%\n",
    "\n",
    "Digit 8 : 9.673809523809524%\n",
    "\n",
    "Digit 9 : 9.971428571428572%\n",
    "\n",
    "On observe ainsi que les données sont réparties dans les bonnes proportions : il y a quasiment autant de 0 que de 1 que de 2, etc ...\n",
    "\n",
    "Mais les images sont-elles réparties de manière uniforme ?\n",
    "Pour le savoir, dressons l'histogramme :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Analyse des données :\n",
    "\n",
    "L'analyse doit permettre de répondre aux questions suivantes :\n",
    "\n",
    "Code TO DO\n",
    "\n",
    "What is the meaning of each feature?\n",
    "Which features are mixed data types?\n",
    "Which features may contain errors or typos?\n",
    "Which features contain blank, null or empty values?\n",
    "What are the data types for various features?\n",
    "What is the distribution of numerical feature values across the samples?\n",
    "The training dataset is balanced or not? What is the main drawback of an unbalanced training dataset?\n",
    "What is the distribution of categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Hypothèses basées sur l'analyse des données\n",
    "\n",
    "On cherche les colonnes qu'on puisse supprimer, ajouter pour gagner du temps de calcul / gagner en qualité de prédiction ?\n",
    "Pour cela, on cherche les corrélations, etc ...\n",
    "\n",
    "Utilisation de PCA ?\n",
    "Normalisation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Préparation des données\n",
    "\n",
    "Code TO DO\n",
    "\n",
    "On met en oeuvre nos hypothèses\n",
    "À la fin de cette partie, nos données doivent être prêtes et dans le bon format pour les passer au modèle de ML choisi (ex : Logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Modèle\n",
    "\n",
    "On choisit un modèle de ML (exemple : Logistic Regression)\n",
    "On justifie pourquoi ce modèle est bien pour notre problème.\n",
    "On regarde quels sont les paramètres les plus adaptés à lui passer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Entraînement et évaluation du modèle\n",
    "\n",
    "Découpage des données.\n",
    "Il y a énormément de lignes, on prend tout ?\n",
    "Validation croisée ?\n",
    "Les données sont-elles mélangées uniformément (censé être déterminé dans l'analyse).\n",
    "\n",
    "Code TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Comparaison de nos résultats avec les résultats qu'on aurait eu sans appliquer nos hypothèses\n",
    "\n",
    "Code TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Prédictions\n",
    "\n",
    "À ce stade notre code est prêt. On l'applique aux données de test (on a pas les réponses, donc cette partie sert juste à dire qu'il faut qu'on le fasse comme dans le cadre d'un projet d'entreprise)\n",
    "\n",
    "Code TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Sauvegarde du modèle dans un fichier\n",
    "\n",
    "Pareil, c'est pour faire les mecs qui pèsent\n",
    "\n",
    "Code TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTEBOOK KERNEL QU'ON SUIT :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88525aed-c0ad-472c-8d2e-2c949a5bd2b3",
    "_uuid": "846a4bfeb71482bc6146299aa7e8b0b8791974d4"
   },
   "source": [
    "## Intoduction\n",
    "link to competition: [digit recogniger competition](https://www.kaggle.com/c/digit-recognizer)\n",
    "\n",
    "My Kaggle Profile: [Amit Vikram | Kaggle](https://www.kaggle.com/amitkvikram)\n",
    "\n",
    "- It gives me immense pleasure to launch this kernel. For your sake of convenience I got accuracy of **0.98442**.\n",
    "\n",
    "- Here we will use PCA for dimensionality reduction and then train the data using \"Logistic Regression with solver lbfgs\" and  \"SVM\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a69922cc-96d5-48ed-8682-7e7437f99fad",
    "_uuid": "213689418e78580706c5db6c6728e97da0c0ed7a"
   },
   "source": [
    "## 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plan :\n",
    "\n",
    "1) On s'inspire du titanic (on ne coule pas) :\n",
    "   on analyse les données à la main pour regarder à chaque colonne :\n",
    "   la moyenne, le nombre de valeurs différentes \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d7d3f5e4-85ac-4709-8fca-b271ee7f8279",
    "_uuid": "c703701cf260091b1eafe3eab2c6a1ec27f13e26"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import warnings\n",
    "\n",
    "Train = pd.read_csv(\"train.csv\").values\n",
    "Test = pd.read_csv(\"test.csv\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "bf2db49d-3d6c-4730-b6ff-d68e8702f547",
    "_uuid": "4e4a10748c2657787c376127ec8bf97417c5fbc1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7884cf9ccb18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Train' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "12e83e9a-5409-4e58-81f7-352b3ed22c53",
    "_uuid": "439f73047e4e7258d8b91bbf68aa6a9a9d96aebe"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Test' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-223d098ab516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Test' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "65bf5b45-a07f-4c84-8b39-6a6d147eb3b1",
    "_uuid": "ac2616dedfa16e143bfca92a12418efb165cef88"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-438dee99d423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Train' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "X = (Train[:,1:])\n",
    "Y = (Train[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d4c88e4-8903-475e-8d32-4008ee5e54e5",
    "_uuid": "4656f32f4da01679dde2e6c971987591124cd648"
   },
   "source": [
    "## 2. Dimensionality Reduction using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56ed120f-46fc-42a1-96e9-5792e28a1f4a",
    "_uuid": "15b7655f2bd79d59864f9aa030d8b102d97e5934"
   },
   "source": [
    "#### a. Plot graph of component vs. cumulative explained variance .\n",
    "This graph will help us in choosing the no of components for training our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "4ec458b5-5c7a-4520-948c-38e9b36e7726",
    "_uuid": "d6121880623ed33c299386d04831a96b0a02193b"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# ## Removing the column with variance 0\n",
    "variance = np.var(X, axis = 0)>1000\n",
    "print(variance.shape)\n",
    "X = X[:, variance]\n",
    "Test = Test[:,variance]\n",
    "print(X.shape)\n",
    "# ##Calculate Principal Components\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "# ##Calculate cumulative explained ration\n",
    "cum_explained_variance = [np.sum(explained_variance[:i+1]) for i in range (0, 201, 25)]\n",
    "X_axis = [i for i in range(0, 201,25)]\n",
    "\n",
    "##Plot Graph\n",
    "fig = plt.figure(figsize = (5.841, 7.195), dpi=100)\n",
    "plt.plot(X_axis, cum_explained_variance, 'ro')\n",
    "plt.grid(True, which = 'both')\n",
    "plt.yticks(cum_explained_variance)\n",
    "plt.xticks(X_axis)\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.xlabel(\"No. of Components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "256448a6-43bb-4ad7-90af-dc38359a8cb7",
    "_uuid": "370a73a0b9d38e88f08c644c33373812c2a713ce"
   },
   "source": [
    "**So Looking on the above graph, 50 components comprise 80% variance. So first we will go with 50 componets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7aa8e43d-d81d-412c-94e6-c7576fa2d3ac",
    "_uuid": "1f4d3b481531a4af55ed9ea27cbde3cdf868d44f"
   },
   "source": [
    "### Splitting data for training and testing\n",
    "- Training data: 80%\n",
    "- Test data: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "7e555966-45d2-4713-8640-6d414bb6e291",
    "_uuid": "f00576135b9504cc80d39b2a3a767b9543e3b9a2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-83006bfb724f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size  = 0.20, random_state  = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28097d9a-29e6-4870-8bac-c996eb773ef5",
    "_uuid": "cab35e9ce318f2ac5c66c0ec1570ce52a0dea03d"
   },
   "source": [
    "### define normalize function for normalizing the data, PrincipalComponents function to return top n principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "87dc2b37-1094-4df3-b720-a1c290b8e3c4",
    "_uuid": "d0b11576f6897d361122ed06c74257b7b7dd41c5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize(sigma2, mean2, X):\n",
    "    X = (X-mean2)/sigma2\n",
    "    return X\n",
    "\n",
    "def PolynomialFeatures1(X):\n",
    "    X_2 = np.square(X)\n",
    "    X = np.column_stack((X, X_2))\n",
    "    return X\n",
    "\n",
    "J1 = []\n",
    "J2 = []\n",
    "\n",
    "##Take n principal components\n",
    "def PrincipalComponents(n):\n",
    "    pca = PCA(n_components= n)\n",
    "    X_train1 = pca.fit_transform(X_train)\n",
    "    X_test1 = pca.transform(X_test)\n",
    "    return X_train1, X_test1\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "def LogisticRegression(X_train2, y_train2, X_test2, y_test2, penalty):\n",
    "    print(\"penalty= \", penalty)\n",
    "    regr = linear_model.LogisticRegression(solver='lbfgs',max_iter=1000, C=penalty)\n",
    "    regr.fit(X_train2, y_train2)\n",
    "    score1 = regr.score(X_train2, y_train2)\n",
    "    score2 = regr.score(X_test2, y_test2)\n",
    "    print(score1, score2)\n",
    "    Prediction = regr.predict(X_test2)\n",
    "    return score1, score2, Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ce08b04f-3e9a-4f60-b100-b1cb8f6c096e",
    "_uuid": "a9bb8cb09e95e5c051a29ee829ede3ace7a39b90"
   },
   "source": [
    "**So usually variance of 0.8 is sufficient to explain the variation in data, so we will first train data by taking the top n principal components which can explaine the variance of 0.8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "1b1e0d3a-e931-42bc-8b27-64e64b729a0c",
    "_uuid": "206b7691f80c4d2d0d666b8571c3993fc82c4768",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1, X_test1 = PrincipalComponents(0.8)   # Getting principal components\n",
    "J1 = []\n",
    "J2 = []\n",
    "for i in range(20000, X_train1.shape[0], 1500):\n",
    "    score1, score2, Prediction = LogisticRegression(X_train1[:i+1,:], y_train[:i+1, ], \n",
    "                                                    X_test1[:i+1,:], y_test[:i+1], 0.1)\n",
    "    J1.append(1-score1)\n",
    "    J2.append(1-score2)\n",
    "    \n",
    "    \n",
    "plt.plot(J2, 'b-', label = \"CV error\")\n",
    "plt.plot(J1, 'r-', label =\"training error\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"No of Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f6a8ad3-c379-4631-88ec-c326c59a1e85",
    "_uuid": "def7c34d94be0b8472ee21a0fe20b8c71de9a5fc"
   },
   "source": [
    "- We got a very irregular graph and not a satisfied accuracy and the accuracy doesn't increase with the increaing training data. So it seems that we are suffering from a high bias. Let us try to train data with top n components explaining the variance of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "8a4c8e47-3371-4907-8a2d-2ff5a8c8a6b0",
    "_uuid": "5b765242b1b78d8d18ccd788ce652a608286a9c9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1, X_test1 = PrincipalComponents(0.9)\n",
    "J1 = []\n",
    "J2 = []\n",
    "for i in range(20000, X_train1.shape[0], 1500):\n",
    "    score1, score2, Prediction = LogisticRegression(X_train1[:i+1,:],\n",
    "                                                    y_train[:i+1, ], X_test1[:i+1,:], y_test[:i+1],0.1)\n",
    "    J1.append(1-score1)\n",
    "    J2.append(1-score2)\n",
    "    \n",
    "    \n",
    "plt.plot(J2, 'b-', label = \"CV error\")\n",
    "plt.plot(J1, 'r-', label =\"training error\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"No of Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39180d2c-5084-4946-9396-fb907976ed14",
    "_uuid": "65507cf388cc47b3d3f94c5f03e85dacb4bdb08c"
   },
   "source": [
    "** We can see that accuracy didn't increase much so it doesn't seems a gud idea. Let us again take top n components explaining the variance of 0.8 and also include thieir polynomials(degree = 2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b84cf435-0bab-40bc-b39e-e4bec724b6ec",
    "_uuid": "a8b4e172d0721b998da8ef2d15dbf422868b606f"
   },
   "source": [
    "#### Add polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebf86ce-2811-4d1b-b589-d4c5da8ab8ae",
    "_uuid": "97aac9ccf64b88cc20a1bec1e7817b4409dde8b8"
   },
   "source": [
    "##### 1.Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "c1097439-f1b0-44d8-a684-210b6100be00",
    "_uuid": "736e43cfcb1289d7e4179a57174737004b14b1d5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(include_bias = False)\n",
    "X_train1, X_test1 = PrincipalComponents(0.81)\n",
    "print(X_test1.shape, X_train1.shape)\n",
    "\n",
    "X_train1 = poly.fit_transform(X_train1)\n",
    "X_test1 = poly.transform(X_test1)\n",
    "\n",
    "print(X_test1.shape, X_train1.shape)\n",
    "\n",
    "sigma = np.std(X_train1, axis = 0)\n",
    "mean = np.mean(X_train1, axis = 0)\n",
    "\n",
    "X_train1 = normalize(sigma , mean , X_train1)\n",
    "X_test1 = normalize(sigma , mean, X_test1)\n",
    "\n",
    "J1 = []\n",
    "J2 = []\n",
    "for i in range(25000, X_train1.shape[0], 1500):\n",
    "    score1, score2, Prediction = LogisticRegression(X_train1[:i+1,:], \n",
    "                                                    y_train[:i+1, ], X_test1[:i+1,:], y_test[:i+1],0.1)\n",
    "    J1.append(1-score1)\n",
    "    J2.append(1-score2)\n",
    "    \n",
    "    \n",
    "plt.plot(J2, 'b-', label = \"CV error\")\n",
    "plt.plot(J1, 'r-', label =\"training error\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"No of Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6ccb3d2-5f7f-49d9-958e-e468e390278c",
    "_uuid": "70f4c14c12d299725cd638f317e19bf9b2226971"
   },
   "source": [
    "** We got way better accuracy here and is data over  fitting .... may be but since we are getting good result on cross validation data also, It's gud to go with this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "090e4276-d3f7-4a2d-8844-9a3dbda14d79",
    "_uuid": "62c956377ea1617f81f03567b83a8829cec6f260"
   },
   "source": [
    "#### Saving data trained with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "562be36a-4f21-407e-a140-4fd4be41ba2e",
    "_uuid": "53efdbcca699d15c29bcf43e3a521d35a9beb6af",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(include_bias = False)\n",
    "\n",
    "X1 = X.copy()\n",
    "Test1 = Test.copy()\n",
    "\n",
    "pca = PCA(n_components= 0.8388)\n",
    "X1 = pca.fit_transform(X1)\n",
    "Test1 = pca.transform(Test)\n",
    "\n",
    "print(X1.shape, Test1.shape)\n",
    "\n",
    "X1 = poly.fit_transform(X1)\n",
    "Test1 = poly.fit_transform(Test1)\n",
    "\n",
    "print(X1.shape, Test1.shape)\n",
    "\n",
    "sigma = np.std(X1, axis = 0)\n",
    "mean = np.mean(X1, axis = 0)\n",
    "\n",
    "X1 = normalize(sigma, mean, X1)\n",
    "Test1 = normalize(sigma , mean, Test1)\n",
    "print(X1.shape, Test1.shape)\n",
    "\n",
    "\n",
    "regr = linear_model.LogisticRegression(solver='lbfgs',max_iter=1000, C=0.1)\n",
    "regr.fit(X1, Y)\n",
    "score1 = regr.score(X1, Y)\n",
    "print(score1)\n",
    "Prediction = regr.predict(Test1)\n",
    "image_id = np.arange(1,Prediction.shape[0]+1)\n",
    "pd.DataFrame({\"ImageId\": image_id, \"Label\": Prediction}).to_csv('out_reg1.csv', \n",
    "                                                                      index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67ea2748-8374-4308-9844-7ca7bf3cedca",
    "_uuid": "85b5fcfa01bf54ae2a52c0d125be67a6629e4204"
   },
   "source": [
    "#### I submitted the data and got an accuracy of 0.9815. Now we will try to train data with SVM and kernel = 'rbf'. Note we will not use polynomial features with SVM since SVM maps the data in higher dimensions so there is no point in including the polynomial features, also SVM doesn't perform well with too many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "896fb162-eb83-4e41-9d88-e8c54fd11b41",
    "_uuid": "7b4f05f69a1f1024e93250c06d7183890fcdb783"
   },
   "source": [
    "##### 2.SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "c05b7039-0587-4f30-bb45-c44d94ee2326",
    "_uuid": "e219b8a43580b09cd276d1667842f9a6bcf3a6f1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def supportVM(X_train2, y_train2, X_test2, y_test2, penalty):\n",
    "    regr = SVC(kernel ='rbf', C=penalty)\n",
    "    regr.fit(X_train2, y_train2)\n",
    "    score1 = regr.score(X_train2, y_train2)\n",
    "    score2 = regr.score(X_test2, y_test2)\n",
    "    print(score1, score2)\n",
    "    Prediction = regr.predict(X_test2)\n",
    "    return score1, score2, Prediction\n",
    "\n",
    "X_train1, X_test1 = PrincipalComponents(0.81)\n",
    "print(X_test1.shape, X_train1.shape)\n",
    "\n",
    "sigma = np.std(X_train1, axis = 0)\n",
    "mean = np.mean(X_train1, axis = 0)\n",
    "\n",
    "X_train1 = normalize(sigma , mean , X_train1)\n",
    "X_test1 = normalize(sigma , mean, X_test1)\n",
    "\n",
    "J1 = []\n",
    "J2 = []\n",
    "for i in range(25000, X_train1.shape[0], 1500):\n",
    "    score1, score2, Prediction = supportVM(X_train1[:i+1,:], \n",
    "                                                    y_train[:i+1, ], X_test1[:i+1,:], y_test[:i+1],10)\n",
    "    J1.append(1-score1)\n",
    "    J2.append(1-score2)\n",
    "    \n",
    "plt.plot(J2, 'b-', label = \"CV error\")\n",
    "plt.plot(J1, 'r-', label =\"training error\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"No of Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f6dee075-618d-404c-a000-f97386ed3706",
    "_uuid": "213f8d2c96984228251f6ace3d12278bc20883e4"
   },
   "source": [
    "- So it seems that this model is performin a little bit better, So we will go with it, Let's save the prediction and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2252dd9f-3855-4085-9700-57f51d9aa3e7",
    "_uuid": "5c4f0ba0e675137874402a241dfe21060a8eb4e2"
   },
   "source": [
    "##### Saving prediction trained with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "d32d8b1d-0874-4289-bcb9-8e6b199a3a0c",
    "_uuid": "7fd4eadd286b560eb468bbf51d50d1b173890fbf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "# poly = PolynomialFeatures(include_bias = False)\n",
    "\n",
    "X1 = X.copy()\n",
    "Test1 = Test.copy()\n",
    "\n",
    "pca = PCA(n_components= 0.8388)\n",
    "X1 = pca.fit_transform(X1)\n",
    "Test1 = pca.transform(Test)\n",
    "\n",
    "print(X1.shape, Test1.shape)\n",
    "\n",
    "print(X1.shape, Test1.shape)\n",
    "\n",
    "sigma = np.std(X1, axis = 0)\n",
    "mean = np.mean(X1, axis = 0)\n",
    "\n",
    "X1 = normalize(sigma, mean, X1)\n",
    "Test1 = normalize(sigma , mean, Test1)\n",
    "print(X1.shape, Test1.shape)\n",
    "\n",
    "\n",
    "regr = SVC(kernel ='rbf', C=10)\n",
    "regr.fit(X1, Y)\n",
    "score1 = regr.score(X1, Y)\n",
    "print(score1)\n",
    "Prediction = regr.predict(Test1)\n",
    "Prediction = regr.predict(Test1)\n",
    "image_id = np.arange(1,Prediction.shape[0]+1)\n",
    "pd.DataFrame({\"ImageId\": image_id, \"Label\": Prediction}).to_csv('out_svm.csv', \n",
    "                                                                      index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "389826f3-5e97-4634-a1dd-5f340547de4e",
    "_uuid": "6d86f34a1ee5ad3c6855578c202f1f24c828530e"
   },
   "source": [
    "** I got an accuracy of 0.98442 on leaderboad with above model**.\n",
    "I am learning CNN, So in future I will use that to train data and most probably I will launch a kernel also .Please give your valuable feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
